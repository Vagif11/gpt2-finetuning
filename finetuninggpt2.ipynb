{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziiuXe5rviG7"
   },
   "source": [
    "in here we are importing necessary libraries such as torch (tenson operations), transformers (to load pretrained models like distilgpt2), dataset(to load standard NLP datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRbIor_awP6i"
   },
   "source": [
    "AutoTokenizer to convert text into token IDs that the model can understand, AutoModelForCausalLM to load a causal language model (`distilgpt2`) suitable for text generation.\n",
    "Trainer and TrainingArguments to fine-tune the model using a high-level training API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y558qOrxwj0V"
   },
   "source": [
    "We used the Hugging Face datasets library to load the wikitext-2-raw-v1 dataset, We split the data into:\n",
    "- train_data â€” used to fine-tune the model.\n",
    "- val_data â€” used for evaluation during training.\n",
    "\n",
    "Since GPT models like `distilgpt2` do not have a default padding token, we use\n",
    "tokenizer.pad_token to match the eos_token and model.config.pad_token_id to ensure padding is handled correctly during training.\n",
    "\n",
    "This step ensures the model doesn't treat padding as meaningful input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset[\"validation\"].map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUPto1ldyg6E"
   },
   "source": [
    "We have a tokenize_function that:\n",
    "- Uses our tokenizer to convert the text into token IDs.\n",
    "- Applies truncation and padding so that all sequences are the same length 128 tokens\n",
    "- Copies the input_ids into a labels field, since causal language modeling uses the same sequence as both input and output (i.e., predict the next token).\n",
    "\n",
    "We applied this function to both the training and validation sets using the `map()` function from Hugging Face Datasets, which efficiently processes the entire dataset in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_distilgpt2\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCrbdIX5zEnu"
   },
   "source": [
    "We used Hugging Face's `Trainer` API to simplify the training process.\n",
    "\n",
    "The `TrainingArguments` class defines key training configurations:\n",
    "- `output_dir`: Folder to save the fine-tuned model.\n",
    "- `num_train_epochs`: We trained for 1 epoch due to time and resource constraints.\n",
    "- `per_device_train_batch_size`: Batch size of 2 was used to fit in limited GPU memory.\n",
    "- `fp16=True`: Enabled 16-bit floating point precision to speed up training on supported GPUs.\n",
    "- `save_steps` and `logging_steps`: Save checkpoints every 500 steps and log every 100 steps.\n",
    "- `report_to=\"none\"`: Prevents integration with third-party tools like WandB.\n",
    "\n",
    "We then passed these arguments into the `Trainer`, along with:\n",
    "- Our model (`distilgpt2`),\n",
    "- The tokenized training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time in a world of artificial intelligence,\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=1.0,         # More creativity\n",
    "    repetition_penalty=1.2,  # Penalize repeats\n",
    "    num_return_sequences=1\n",
    "\n",
    ")\n",
    "print(\"ðŸ“ Generated Text:\\n\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhnpTvmizPju"
   },
   "source": [
    "After training, we evaluated the model by generating text based on a custom prompt:\n",
    "> \"Once upon a time in a world of artificial intelligence,\"\n",
    "\n",
    "The `tokenizer` converts the prompt into tokens and `model.generate()` creates a continuation.\n",
    "\n",
    "We used advanced generation parameters to improve quality:\n",
    "- `max_length=50`: Allows the model to generate a full paragraph.\n",
    "- `do_sample=True`: Enables randomness in output.\n",
    "- `top_k=50` and `top_p=0.95`: Apply nucleus sampling for more diverse but controlled text.\n",
    "- `temperature=1.0`: Adds creativity by allowing a wider range of word choices.\n",
    "- `repetition_penalty=1.2`: Discourages the model from repeating phrases.\n",
    "\n",
    "Finally, we decoded the output tokens back to text and printed the result.  \n",
    "Setting `pad_token_id` to `eos_token_id` ensured smooth generation without warnings.\n",
    "\n",
    "The result was a coherent and imaginative paragraph that reflected the language style of the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK7nMGJSzYut"
   },
   "source": [
    "We fine-tuned the distilgpt2 model using the wikitext-2-raw-v1 dataset from Hugging Face. The dataset was tokenized with a maximum sequence length of 128, and we trained for 1 epoch using Hugging Faceâ€™s Trainer API. During training, we used a batch size of 2, 16-bit floating point precision (fp16), and saved the model locally. After training, we used generate() to produce text based on custom prompts and evaluated the quality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
